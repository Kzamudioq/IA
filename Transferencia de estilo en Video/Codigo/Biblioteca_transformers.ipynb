{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM6ss1eHJ3iW",
        "outputId": "0c6836ee-7088-480d-a5de-a3591a3118ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "#Configurar el entorno en Colab e instalación de librerias\n",
        "\n",
        "!pip install tensorflow numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de Librerías\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing import image as kp_image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import cv2"
      ],
      "metadata": {
        "id": "ixDjLPYNMIE8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "content_layers = ['block5_conv2']\n",
        "num_style_layers = len(style_layers)\n",
        "num_content_layers = len(content_layers)"
      ],
      "metadata": {
        "id": "OoKZUPJFNuBi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones Auxiliares\n",
        "\n",
        "def load_and_process_img(path_to_img):\n",
        "    img = kp_image.load_img(path_to_img)\n",
        "    img = kp_image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_img(processed_img):\n",
        "    x = processed_img.copy()\n",
        "    if len(x.shape) == 4:\n",
        "        x = np.squeeze(x, 0)\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def show_img(img, title=None):\n",
        "    if len(img.shape) == 4:\n",
        "        img = np.squeeze(img, 0)\n",
        "    img = deprocess_img(img)\n",
        "    plt.imshow(img)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CFp7K1jtKd_e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utiliza el modelo VGG19 preentrenado para extraer las características necesarias:\n",
        "\n",
        "def get_model():\n",
        "    vgg = VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    selected_layers = style_layers + content_layers\n",
        "    outputs = [vgg.get_layer(name).output for name in selected_layers]\n",
        "    model = Model([vgg.input], outputs)\n",
        "    return model\n",
        "\n",
        "def get_feature_representations(model, content_path, style_path):\n",
        "    content_image = load_and_process_img(content_path)\n",
        "    style_image = load_and_process_img(style_path)\n",
        "\n",
        "    style_outputs = model(style_image)\n",
        "    content_outputs = model(content_image)\n",
        "\n",
        "    style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
        "    content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
        "\n",
        "    return style_features, content_features"
      ],
      "metadata": {
        "id": "2IfWM852Km1Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las Funciones de Pérdida y el Algoritmo de Optimización\n",
        "\n",
        "def compute_content_loss(base_content, target):\n",
        "    return tf.reduce_mean(tf.square(base_content - target))\n",
        "\n",
        "def gram_matrix(input_tensor):\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def compute_style_loss(base_style, gram_target):\n",
        "    height, width, channels = base_style.get_shape().as_list()\n",
        "    gram_style = gram_matrix(base_style)\n",
        "\n",
        "    return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
        "\n",
        "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
        "    style_weight, content_weight = loss_weights\n",
        "\n",
        "    model_outputs = model(init_image)\n",
        "\n",
        "    style_output_features = model_outputs[:num_style_layers]\n",
        "    content_output_features = model_outputs[num_style_layers:]\n",
        "\n",
        "    style_score = 0\n",
        "    content_score = 0\n",
        "\n",
        "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
        "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
        "        style_score += weight_per_style_layer * compute_style_loss(comb_style[0], target_style)\n",
        "\n",
        "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "    for target_content, comb_content in zip(content_features, content_output_features):\n",
        "        content_score += weight_per_content_layer * compute_content_loss(comb_content[0], target_content)\n",
        "\n",
        "    style_score *= style_weight\n",
        "    content_score *= content_weight\n",
        "\n",
        "    loss = style_score + content_score\n",
        "    return loss, style_score, content_score\n",
        "\n",
        "@tf.function()\n",
        "def compute_grads(cfg):\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_loss = compute_loss(**cfg)\n",
        "    total_loss = all_loss[0]\n",
        "    return tape.gradient(total_loss, cfg['init_image']), all_loss"
      ],
      "metadata": {
        "id": "IhoxCruqKz4g"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función principal para realizar la transferencia de estilo\n",
        "\n",
        "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
        "    model = get_model()\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
        "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "    init_image = load_and_process_img(content_path)\n",
        "    init_image = tf.Variable(init_image, dtype=tf.float32)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=5.0, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    iter_count = 1\n",
        "    best_loss, best_img = float('inf'), None\n",
        "\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    cfg = {\n",
        "        'model': model,\n",
        "        'loss_weights': loss_weights,\n",
        "        'init_image': init_image,\n",
        "        'gram_style_features': gram_style_features,\n",
        "        'content_features': content_features\n",
        "    }\n",
        "\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        grads, all_loss = compute_grads(cfg)\n",
        "        loss, style_score, content_score = all_loss\n",
        "        opt.apply_gradients([(grads, init_image)])\n",
        "        clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
        "        init_image.assign(clipped)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_img = deprocess_img(init_image.numpy())\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration: {}'.format(i))\n",
        "            print('Total loss: {:.4e}, style loss: {:.4e}, content loss: {:.4e}'.format(loss, style_score, content_score))\n",
        "\n",
        "    return best_img, best_loss"
      ],
      "metadata": {
        "id": "tFDD1WUUMj22"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = '/content/25-255961_kaori-shigatsu-wa-kimi-no-uso-hd.jpg'\n",
        "style_path = '/content/Shigatsu.wa.Kimi.no.Uso.full.1828184.jpg'\n",
        "\n",
        "best_img, best_loss = run_style_transfer(content_path, style_path, num_iterations=1000)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "show_img(best_img, 'Output Image')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmBiH0Q6MtlO",
        "outputId": "75f04341-6f20-48a3-9533-8eb2a5cf276f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "Total loss: 1.4059e+08, style loss: 1.4059e+08, content loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = 'ruta/a/tu/imagen_de_contenido.jpg'\n",
        "style_path = 'ruta/a/tu/imagen_de_estilo.jpg'\n",
        "num_iterations = 1000\n",
        "video_filename = 'output_video.mp4'\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "height, width, _ = load_and_process_img(content_path).shape[1:]\n",
        "video = cv2.VideoWriter(video_filename, fourcc, 20.0, (width, height))\n",
        "\n",
        "model = get_model()\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
        "gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "init_image = load_and_process_img(content_path)\n",
        "init_image = tf.Variable(init_image, dtype=tf.float32)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=5.0, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "loss_weights = (1e-2, 1e3)\n",
        "cfg = {\n",
        "    'model': model,\n",
        "    'loss_weights': loss_weights,\n",
        "    'init_image': init_image,\n",
        "    'gram_style_features': gram_style_features,\n",
        "    'content_features': content_features\n",
        "}\n",
        "\n",
        "norm_means = np.array([103.939, 116.779, 123.68])\n",
        "min_vals = -norm_means\n",
        "max_vals = 255 - norm_means\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    grads, all_loss = compute_grads(cfg)\n",
        "    opt.apply_gradients([(grads, init_image)])\n",
        "    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
        "    init_image.assign(clipped)\n",
        "\n",
        "    if i % 10 == 0:  # Guardar una imagen cada 10 iteraciones\n",
        "        frame = deprocess_img(init_image.numpy())\n",
        "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print('Iteration: {}'.format(i))\n",
        "\n",
        "video.release()"
      ],
      "metadata": {
        "id": "5nxuqg8jNXcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}